# ğŸ–¼ï¸ Image Captioning with VGG16

This project implements an **image captioning model** using deep learning, trained on the **Flickr8k dataset**. The goal is to generate captions for images by combining visual and textual data.

## ğŸ“Œ What the Project Does

- Takes an image as input
- Extracts visual features using **VGG16**
- Processes captions using **Embedding + LSTM**
- Merges both inputs to generate a caption word-by-word

## ğŸ§  Model Architecture

- **Image Encoder**: VGG16 + Dense layers
- **Caption Sequence Encoder**: Embedding â†’ Dropout â†’ LSTM
- **Decoder**: Fully connected layers to predict next word

## ğŸ“Š Results

- **BLEU-1 Score**: 0.5402  
- **BLEU-2 Score**: 0.3247

## ğŸ§° Tools Used

- Python
- Keras (TensorFlow backend)
- Flickr8k Dataset
- VGG16
- LSTM

## ğŸ–¼ï¸ Sample Output

Each image is shown with:
- Actual captions (from dataset)
- Predicted caption (generated by model)

## ğŸ“Œ Future Improvements

- Add Attention Mechanism
- Use Beam Search for better prediction
- Train on larger datasets like MS COCO
