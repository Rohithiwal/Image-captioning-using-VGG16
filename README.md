# 🖼️ Image Captioning with VGG16

This project implements an **image captioning model** using deep learning, trained on the **Flickr8k dataset**. The goal is to generate captions for images by combining visual and textual data.

## 📌 What the Project Does

- Takes an image as input
- Extracts visual features using **VGG16**
- Processes captions using **Embedding + LSTM**
- Merges both inputs to generate a caption word-by-word

## 🧠 Model Architecture

- **Image Encoder**: VGG16 + Dense layers
- **Caption Sequence Encoder**: Embedding → Dropout → LSTM
- **Decoder**: Fully connected layers to predict next word

## 📊 Results

- **BLEU-1 Score**: 0.5402  
- **BLEU-2 Score**: 0.3247

## 🧰 Tools Used

- Python
- Keras (TensorFlow backend)
- Flickr8k Dataset
- VGG16
- LSTM

## 🖼️ Sample Output

Each image is shown with:
- Actual captions (from dataset)
- Predicted caption (generated by model)

## 📌 Future Improvements

- Add Attention Mechanism
- Use Beam Search for better prediction
- Train on larger datasets like MS COCO
